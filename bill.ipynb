{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72aca632-bab1-4e11-90be-b16eb1fc41b4",
   "metadata": {},
   "source": [
    "# Partie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c394be9-e4fa-40b8-bd91-5acd61bbaa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "def convert_to_numbers(s):\n",
    "   \n",
    "    words_to_numbers = {\n",
    "        'one': '1',\n",
    "        'two': '2',\n",
    "        'three': '3',\n",
    "        'four': '4',\n",
    "        'five': '5',\n",
    "        'six': '6',\n",
    "        'seven': '7',\n",
    "        'eight': '8',\n",
    "        'nine': '9',\n",
    "        'zero': '0'\n",
    "    }\n",
    " \n",
    "    pattern = re.compile(r'\\b(' + '|'.join(words_to_numbers.keys()) + r')\\b')\n",
    "    return re.sub(pattern, lambda x: words_to_numbers[x.group()], s)\n",
    "def reg_num(num_str):\n",
    "    try:\n",
    "        return float(num_str)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return float(num_str.replace(',', '.'))\n",
    "        except ValueError:\n",
    "            return None  # Return None for invalid numbers\n",
    "class Product:\n",
    "    def __init__(self, qnt, price, name):\n",
    "        self.qnt = reg_num(qnt) \n",
    "        self.UnitPrice =reg_num(price) \n",
    "        self.name = name\n",
    "\n",
    "class BillGenerator:\n",
    "    def __init__(self, text):\n",
    "        self.bill = []\n",
    "        self.text = text\n",
    "        print(self.text)\n",
    "        self.sentences=self.split_text_into_sentences()\n",
    "        for sentence in self.sentences:\n",
    "            nums = self.findNum(sentence)\n",
    "            # Find the positions of \"START\" and \"END\" in the string\n",
    "            start_match = re.search(nums[0], sentence)\n",
    "            end_match = re.search(nums[1], sentence)\n",
    "    # Extract the substrings between \"START\" and \"END\"\n",
    "            start_index = start_match.end()\n",
    "            end_index = end_match.start()\n",
    "            substr_between = sentence[start_index:end_index]\n",
    "            names = self.findName(substr_between)\n",
    "            names = ' '.join(names)\n",
    "            nums = [float(num) if '.' in num else convert_to_numbers(num) for num in nums]\n",
    "            \n",
    "            self.bill.append(Product(nums[0], nums[1], names))\n",
    "            \n",
    "\n",
    "    def printBill(self):\n",
    "        # Print header\n",
    "        print(\"Product\\tQuantity\\tUnitPrice\\tTotalPrice\")\n",
    "        # Iterate over each Product in the bill\n",
    "        for product in self.bill:\n",
    "            # Calculate total price\n",
    "            total_price = product.qnt * product.UnitPrice\n",
    "            # Print product details\n",
    "            print(f\"{product.name}\\t{product.qnt}\\t\\t{product.UnitPrice}\\t\\t{total_price}\")\n",
    "    def toDF(self):\n",
    "    # Create a list to hold the data for each product\n",
    "        bill_data = []\n",
    "    \n",
    "    # Iterate over each Product in the bill\n",
    "        for product in self.bill:\n",
    "        # Calculate total price\n",
    "            total_price = product.qnt * product.UnitPrice\n",
    "        # Append product details to the list\n",
    "            bill_data.append([product.name, product.qnt, product.UnitPrice, total_price])\n",
    "    \n",
    "    # Create a DataFrame from the bill data\n",
    "        bill_df = pd.DataFrame(bill_data, columns=['Product', 'Quantity', 'UnitPrice', 'TotalPrice'])\n",
    "    \n",
    "    # Print the DataFrame\n",
    "        return bill_df\n",
    "\n",
    "\n",
    "    def split_text_into_sentences(self):\n",
    "    # Define the regular expression pattern to split the text at commas\n",
    "        sentence_pattern = r',(?!\\d)\\s*|and\\s*'\n",
    "    \n",
    "    # Split the text into sentences using the pattern\n",
    "        sentences = re.split(sentence_pattern, self.text)\n",
    "    \n",
    "    # Remove any empty strings or whitespace-only strings from the list\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "        return sentences\n",
    "        \n",
    "    def findName(self,sentence):\n",
    "        product_name_pattern = r'\\b(?!kilo|fresh|new|your|about|also|with|for|with|also|mil|dollar)\\b[A-Za-z]{3,}\\b'  # Regular expression pattern for product name\n",
    "        product_names = re.findall(product_name_pattern, sentence)\n",
    "        return product_names\n",
    "\n",
    "    def findNum(self,sentence):\n",
    "        number_pattern = r\"\\b(\\b\\d+(?:,\\d+)*(?:\\.\\d+)?(?:\\d+(?:,\\d+)*(?:\\.\\d+)?)?\\b|(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion|trillion))\\b\"\n",
    "    \n",
    "    # Use the re.findall() function to find all numbers matching the pattern\n",
    "        numbers = re.findall(number_pattern, sentence)\n",
    "        if len(numbers) < 2:\n",
    "            numbers[0] , numbers[1] = 1 , numbers[0]\n",
    "            print(\"here\")\n",
    "        sentence = re.sub(number_pattern, '', sentence)\n",
    "    # Convert the numbers to floats\n",
    "       # numbers = [float(num) if '.' in num else convert_to_numbers(num) for num in numbers]\n",
    "    \n",
    "        return numbers\n",
    "        \n",
    "    #def findQnt(self):\n",
    "    #def findUnitPrice(self):\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fc399b-ce58-461d-9455-a652f9534e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought three Samsung smartphones 150 $ each, four kilos of fresh banana for 1,2 dollar a kilogram and one Hamburger with 4,5 dollar\n"
     ]
    }
   ],
   "source": [
    "bill = BillGenerator(\"I bought three Samsung smartphones 150 $ each, four kilos of fresh banana for 1,2 dollar a kilogram and one Hamburger with 4,5 dollar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e966f01-7236-4c22-947e-05b602883de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\tQuantity\tUnitPrice\tTotalPrice\n",
      "Samsung smartphones\t3.0\t\t150.0\t\t450.0\n",
      "banana\t4.0\t\t1.2\t\t4.8\n",
      "Hamburger\t1.0\t\t4.5\t\t4.5\n"
     ]
    }
   ],
   "source": [
    "bill.printBill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b82cb1-10c6-4e8f-81c7-81441acc9a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>TotalPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung smartphones</td>\n",
       "      <td>3.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hamburger</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Product  Quantity  UnitPrice  TotalPrice\n",
       "0  Samsung smartphones       3.0      150.0       450.0\n",
       "1               banana       4.0        1.2         4.8\n",
       "2            Hamburger       1.0        4.5         4.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a8358f-cd90-464c-aedd-458b50a39a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yesterday i bought from my friend one jersey for 3 dollars and not oky thay but also one new hat for 5\n",
      "Product\tQuantity\tUnitPrice\tTotalPrice\n",
      "jersey\t1.0\t\t3.0\t\t3.0\n",
      "hat\t1.0\t\t5.0\t\t5.0\n"
     ]
    }
   ],
   "source": [
    "bill = BillGenerator(\"yesterday i bought from my friend one jersey for 3 dollars and not oky thay but also one new hat for 5\")\n",
    "bill.printBill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7895503-7ee3-4150-9311-2a6e35f1a9d8",
   "metadata": {},
   "source": [
    "# Partie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed78bdeb-9c40-4d40-883d-ceb826dbd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87538d93-11e7-47eb-a1b4-359e363c45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "# Connect to MongoDB (assuming it's running on localhost)#\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "# Select database\n",
    "db = client['NLP']\n",
    "\n",
    "# Select collection\n",
    "collection = db['TP1']\n",
    "# Retrieve all documents from the collection\n",
    "data = [article['article_text'] for article in collection.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a7d6b3-a216-4888-a52a-72bc6407d3ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['تركيا توقف التبادل التجاري مع إسرائيل بسبب \"المأساة الإنسانية\" في غزة', '', 'علقت تركيا جميع معاملاتها التجارية مع إسرائيل بسبب حربها على غزة، مشيرة إلى \"تفاقهم المأساة الإنسانية\" في القطاع.', 'وقالت وزارة التجارة التركية إن هذه الإجراءات ستبقى سارية المفعول إلى أن تسمح إسرائيل بوصول المساعدات إلى غزة، بشكل \"كاف وغير متقطع\".', 'وبلغ حجم المبادلات التجارية بين البلدين 7 مليارات دولار، العام الماضي.', 'واتهم وزير الخارجية الإسرائيلي يسرائيل كاتس، الرئيس التركي رجب طيب أردوغان، بأنه يتصرف مثل \"الدكتاتور\".', 'وقال كاتس، على موقع أكس، إن أردوغان \"يغفل مصلحة الشعب التركي، ومصلحة رجال الأعمال الأتراك، ويتجاهل الاتفاقات التجارية الدولية\".', 'وأضاف أنه أعطى تعليمات إلى وزارة الخارجية بأن تبحث عن بدائل للتجارة مع تركيا، بالتركيز على الانتاج المحلي، والاستيراد من دول أخرى.', 'وجاء في البيان التركي أن التعليق يشمل \"جميع المنتجات\".', 'شرح معمق لقصة بارزة من أخباراليوم، لمساعدتك على فهم أهم الأحداث حولك وأثرها على حياتك', 'الحلقات', 'يستحق الانتباه نهاية', 'وأضاف: \"تركيا ستنفذ هذه الإجراءات الجديدة بصرامة وحزم، إلى أن تسمح الحكومة الإسرائيلية بصول المساعدات إلى غزة بشكل كاف وغير متقطع\".', 'وكانت تركيا في 1949، أول دولة، ذات أغلبية مسلمة، تعترف بإسرائيل. لكن العلاقات تفاقمت بين البلدين في العقود الأخيرة.', 'ففي 2010، قطعت تركيا العلاقات الدبلوماسية مع إسرائيل، بعد مقتل 10 ناشطين أتراك مساندين لفلسطين، على يد فرقة من الجيش الإسرائيلي، اقتحمت باخرتهم، وهي تحاول كسر الحصار البحري الإسرائيلي، المفروض على غزة.', 'وعادت العلاقات الدبلوماسية بين البلدين، إلى سابق عهدها، في 2016، لكن كلا منهما طرد سفير الدولة الأخرى، بعد عامين من ذلك، بسبب خلاف بشأن مقتل فلسطينيين على يد إسرائيل، في احتجاجات على حدود قطاع غزة.', 'وتزايد انتقاد أردوغان لإسرائيل منذ حملتها العسكرية الأخيرة على قطاع غزة، عقب هجوم حماس يوم 7 أكتوبر/تشرين الأول.', 'وقال أردوغان في يناير/كانون الثاني، إن الحملة العسكرية، التي شنها رئيس الوزراء، بنيامين نيتنياهو، ردا على الهجوم، لا تقل في شيء \"عما فعله هتلر\".', 'ورد نتنياهو بالقول إن \"إردوغان الذي يرتكب الإبادة الجماعية ضد الأكراد، ويحمل رقما قياسيا في سجن الصحفيين، الذين يعارضون حكمه، آخر من يحق له أن يعطينا موعظة في الآخلاق\".', 'وتواجه إسرائيل انتقادات متزايدة بشأن الأوضاع في قطاع غزة. فقدت كشفت تقارير، مدعومة من الأمم المتحدة، الشهر الماضي، أن 1.1 مليون شخص يعانون من الجوع في غزة، وأن المجاعة ستحل في شمال القطاع في شهر مايو/أيار.', '', 'وقال البيت الأبيض الخميس إن رصيفا بحريا، يبنيه الجيش الأمريكي، لتسهيل وصول المساعدات إلى قطاع غزة، سيفتح خلال أيام.', 'ونشرت الولايات المتحدة صورا لبواخر الإمداد والجنود يعملون على تركيب الرصيف البحري، من قطع حديدية، على مقربة من سفينة تابعة للبحرية الأمريكية.', 'لكن الأمم المتحدة قالت إن المعابر البحرية لا يمكنها أن تعوض الطرق البرية، التي يصل من خلالها أكبر حجم من المساعدات الإنسانية.', 'وفي مطلع هذا الأسبوع، فتحت إسرائيل معبر إيريز، إلى شمال غزة، لدخول قوافل المساعدات، بعد ضغوط من حلفائها الغربيين، وبسبب المطالبات المتكررة من المنظمات الإنسانية.', 'لكن الأردن قال إن بعض شاحناته تعرضت للهجوم من قبل المستوطنين الإسرائيليين، قبل وصولها إلى المعبر.', 'وكشف تقرير مدعوم من الأمم المتحدة أخيرا إحصائيات تدل على أن الكارثة الإنسانية في غزة في طريقها إلى أن تصبح مجاعة من صنع الإنسان.', 'وقال كبير المسؤولين عن حقوق الإنسان في الأمم المتحدة، فولكر ترك، لبي بي سي إن هناك \"ما يجعلنا نعتقد\" أن إسرائيل تستعمل التجويع كسلاح في الحرب.', 'وتنفي إسرائيل أنها تعطل وصول المساعدات الإنسانية. وألقت باللوم على الأمم المتحدة في عدم تمكنها من توزيع الإعانات على المحتاجين داخل غزة.', 'وشنت إسرائيل حملة عسكرية على قطاع غزة \"لتدمير حماس\"، ردا على هجوم حركة المقاومة الإسلامية على إسرائيل يوم 7 أكتوبر/تشرين الأول. وأسفر الهجوم، حسب الحكومة الإسرائيلية، عن مقتل 1200 شخص، و253 رهينة.', 'وقتلت إسرائيل، منذ بداية حملتها العسكرية حتى الآن 34,500 فلسطيني في غزة، حسب وزارة الصحة في القطاع.', 'وتوجه وزير الخارجية الأمريكي أنتوني بليكين، الأربعاء، إلى إسرائيل وحماس بقوله إن \"الوقت حان الآن\"، للتوصل إلى اتفاق بوقف إطلاق النار في غزة، والإفراج عن الرهائن. وقال إن الاتفاق على الطاولة وأنه على حماس أن توافق عليه.', 'وينتظر الوسطاء رد حماس على المقترحات الأخيرة.', 'وذكرت التقارير أنه يتضمن وقفا لإطلاق النار لمدة 40 يوما، والإفراج عن أكثر من 30 من الرهائن الإسرائيليين، مقابل إطلاق سرح عدد أكبر من السجناء والمعتقلين الفلسطينيين.', '', '', '', '', '', '', '', '', '', '', '\"عشر دقائق فقط، لو تأخرت لما تمكنت من إخباركم قصتي اليوم\" مراسل بي بي سي في غزة', 'كيف تتصرف إذا علقت داخل المصعد مع انقطاع الكهرباء؟', 'اجتياح إسرائيل لرفح قد يكون \"خدعة\" أو مقدمة لحرب مدمرة', '\"لا حيلة ولا قوة لنا\": اللاجئون السوريون في الأردن ولبنان و\"العودة الآمنة\"', 'حرب السودان: تحذيرات من كارثة وشيكة في مدينة الفاشر المحاصرة ', 'مُحاكمة \"مليئة بالتساؤلات\"، وخيارات متاحة بشأن حصانة ترامب في قضية تدخله بالانتخابات', '', '2024 بي بي سي. بي بي سي ليست مسؤولة عن محتوى المواقع الخارجية.\\n \\nسياستنا بخصوص الروابط الخارجية.']\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d55866-26c5-4b20-a2d6-c160261393c7",
   "metadata": {},
   "source": [
    "## 1. one hot encoding, bag of words, TF-IDF technics on the Data vectors collected during the lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73fb44e8-d096-4229-ae04-0b0260fabb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoded shape:   (0, 5479)\t1\n",
      "  (0, 5994)\t1\n",
      "  (0, 1924)\t1\n",
      "  (0, 1930)\t1\n",
      "  (0, 10046)\t1\n",
      "  (0, 938)\t1\n",
      "  (0, 4787)\t1\n",
      "  (0, 3378)\t1\n",
      "  (0, 1654)\t1\n",
      "  (0, 8163)\t1\n",
      "  (0, 7902)\t1\n",
      "  (0, 7777)\t1\n",
      "  (0, 6158)\t1\n",
      "  (0, 10060)\t1\n",
      "  (0, 1931)\t1\n",
      "  (0, 6306)\t1\n",
      "  (0, 7786)\t1\n",
      "  (0, 9983)\t1\n",
      "  (0, 1000)\t1\n",
      "  (0, 5722)\t1\n",
      "  (0, 3232)\t1\n",
      "  (0, 12161)\t1\n",
      "  (0, 11936)\t1\n",
      "  (0, 1929)\t1\n",
      "  (0, 1970)\t1\n",
      "  :\t:\n",
      "  (67, 4314)\t1\n",
      "  (67, 853)\t1\n",
      "  (67, 10731)\t1\n",
      "  (67, 2898)\t1\n",
      "  (67, 5565)\t1\n",
      "  (67, 5900)\t1\n",
      "  (67, 211)\t1\n",
      "  (67, 264)\t1\n",
      "  (67, 179)\t1\n",
      "  (67, 233)\t1\n",
      "  (67, 3637)\t1\n",
      "  (67, 3804)\t1\n",
      "  (67, 4986)\t1\n",
      "  (67, 4163)\t1\n",
      "  (67, 242)\t1\n",
      "  (67, 203)\t1\n",
      "  (67, 8042)\t1\n",
      "  (67, 4748)\t1\n",
      "  (67, 9359)\t1\n",
      "  (67, 1890)\t1\n",
      "  (67, 5373)\t1\n",
      "  (67, 9627)\t1\n",
      "  (67, 222)\t1\n",
      "  (67, 8698)\t1\n",
      "  (67, 5864)\t1\n",
      "Bag of Words shape:   (0, 5479)\t6\n",
      "  (0, 5994)\t1\n",
      "  (0, 1924)\t1\n",
      "  (0, 1930)\t1\n",
      "  (0, 10046)\t5\n",
      "  (0, 938)\t14\n",
      "  (0, 4787)\t3\n",
      "  (0, 3378)\t2\n",
      "  (0, 1654)\t6\n",
      "  (0, 8163)\t27\n",
      "  (0, 7902)\t17\n",
      "  (0, 7777)\t2\n",
      "  (0, 6158)\t2\n",
      "  (0, 10060)\t1\n",
      "  (0, 1931)\t3\n",
      "  (0, 6306)\t1\n",
      "  (0, 7786)\t22\n",
      "  (0, 9983)\t1\n",
      "  (0, 1000)\t13\n",
      "  (0, 5722)\t1\n",
      "  (0, 3232)\t3\n",
      "  (0, 12161)\t1\n",
      "  (0, 11936)\t3\n",
      "  (0, 1929)\t1\n",
      "  (0, 1970)\t1\n",
      "  :\t:\n",
      "  (67, 4314)\t1\n",
      "  (67, 853)\t1\n",
      "  (67, 10731)\t1\n",
      "  (67, 2898)\t1\n",
      "  (67, 5565)\t1\n",
      "  (67, 5900)\t1\n",
      "  (67, 211)\t1\n",
      "  (67, 264)\t1\n",
      "  (67, 179)\t1\n",
      "  (67, 233)\t1\n",
      "  (67, 3637)\t1\n",
      "  (67, 3804)\t1\n",
      "  (67, 4986)\t1\n",
      "  (67, 4163)\t1\n",
      "  (67, 242)\t1\n",
      "  (67, 203)\t3\n",
      "  (67, 8042)\t2\n",
      "  (67, 4748)\t2\n",
      "  (67, 9359)\t1\n",
      "  (67, 1890)\t1\n",
      "  (67, 5373)\t1\n",
      "  (67, 9627)\t2\n",
      "  (67, 222)\t2\n",
      "  (67, 8698)\t2\n",
      "  (67, 5864)\t1\n",
      "TF-IDF shape:   (0, 2562)\t0.009094955122962042\n",
      "  (0, 4694)\t0.009094955122962042\n",
      "  (0, 7131)\t0.009094955122962042\n",
      "  (0, 3941)\t0.009094955122962042\n",
      "  (0, 9711)\t0.009094955122962042\n",
      "  (0, 9867)\t0.009094955122962042\n",
      "  (0, 9440)\t0.009094955122962042\n",
      "  (0, 85)\t0.008963923271011472\n",
      "  (0, 4405)\t0.0122509234807342\n",
      "  (0, 5417)\t0.0122509234807342\n",
      "  (0, 8314)\t0.0122509234807342\n",
      "  (0, 5446)\t0.0122509234807342\n",
      "  (0, 6338)\t0.0122509234807342\n",
      "  (0, 9578)\t0.0122509234807342\n",
      "  (0, 11868)\t0.0122509234807342\n",
      "  (0, 4420)\t0.0122509234807342\n",
      "  (0, 10236)\t0.0122509234807342\n",
      "  (0, 6239)\t0.0122509234807342\n",
      "  (0, 3499)\t0.0122509234807342\n",
      "  (0, 3091)\t0.0122509234807342\n",
      "  (0, 9793)\t0.011877716004019613\n",
      "  (0, 12013)\t0.0122509234807342\n",
      "  (0, 8396)\t0.0122509234807342\n",
      "  (0, 5359)\t0.0122509234807342\n",
      "  (0, 2690)\t0.0122509234807342\n",
      "  :\t:\n",
      "  (67, 7722)\t0.013673234237829856\n",
      "  (67, 7791)\t0.045995463532153065\n",
      "  (67, 6550)\t0.013055031136544564\n",
      "  (67, 7122)\t0.11165949829970996\n",
      "  (67, 5130)\t0.2131681331176281\n",
      "  (67, 1397)\t0.013673234237829856\n",
      "  (67, 12147)\t0.0175571129334872\n",
      "  (67, 6513)\t0.034486050017808016\n",
      "  (67, 8660)\t0.013256698026788062\n",
      "  (67, 2121)\t0.025328008914312004\n",
      "  (67, 4808)\t0.013462702170207112\n",
      "  (67, 6680)\t0.04664471096466335\n",
      "  (67, 4874)\t0.04232613088153519\n",
      "  (67, 10266)\t0.13585293902892814\n",
      "  (67, 7828)\t0.030452590445375446\n",
      "  (67, 10420)\t0.0689931952982296\n",
      "  (67, 2360)\t0.02030172696358363\n",
      "  (67, 10766)\t0.04440437143403686\n",
      "  (67, 1000)\t0.01124231542803629\n",
      "  (67, 7786)\t0.04180090431659328\n",
      "  (67, 7777)\t0.013673234237829856\n",
      "  (67, 7902)\t0.013055031136544564\n",
      "  (67, 8163)\t0.09135777133612634\n",
      "  (67, 938)\t0.013673234237829856\n",
      "  (67, 10046)\t0.013055031136544564\n"
     ]
    }
   ],
   "source": [
    "# Apply one hot encoding\n",
    "flattened_data = [' '.join(doc) for doc in data]\n",
    "cv = CountVectorizer(binary=True)\n",
    "one_hot_encoded = cv.fit_transform(flattened_data)\n",
    "\n",
    "# Apply bag of words\n",
    "cv = CountVectorizer()\n",
    "bag_of_words = cv.fit_transform(flattened_data)\n",
    "\n",
    "# Apply TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(flattened_data)\n",
    "\n",
    "# Print the shapes of the matrices\n",
    "print(\"One Hot Encoded shape:\", one_hot_encoded)\n",
    "print(\"Bag of Words shape:\", bag_of_words)\n",
    "print(\"TF-IDF shape:\", tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbe2aa-462b-4c51-ba2b-f6d14cf7bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_visualize = 'فلسطين'\n",
    "\n",
    "tokenized_data = []\n",
    "for article in flattened_data:\n",
    "    tokenized_article = word_tokenize(article)\n",
    "    tokenized_article = [word if word != 'غزة' else word_to_visualize for word in tokenized_article]\n",
    "    tokenized_data.append(tokenized_article)\n",
    "\n",
    "# Apply Skip Gram\n",
    "\n",
    "skipgram_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, sg=1)\n",
    "\n",
    "# Apply CBOW\n",
    "cbow_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, sg=0)\n",
    "\n",
    "# Example of accessing word vectors\n",
    "print(\"Word Vector for '{}' (Skip Gram):\\n\".format(word_to_visualize), skipgram_model.wv[word_to_visualize])\n",
    "print(\"Word Vector for '{}' (CBOW):\\n\".format(word_to_visualize), cbow_model.wv[word_to_visualize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9ceee-c4a9-4137-bf86-4fd7a9a03909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Apply Glove\n",
    "glove_model = FastText(sentences=tokenized_data, vector_size=100, window=5, sg=1)\n",
    "\n",
    "# Apply FastText\n",
    "fasttext_model = FastText(sentences=tokenized_data, vector_size=100, window=5, sg=0)\n",
    "\n",
    "# Example of accessing word vectors\n",
    "print(\"Word Vector for '{}' (GloVe):\\n\".format(word_to_visualize), glove_model.wv[word_to_visualize])\n",
    "print(\"Word Vector for '{}' (FastText):\\n\".format(word_to_visualize), fasttext_model.wv[word_to_visualize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907924d-0624-408c-a133-db9790fba378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot vectors using t-SNE in 3D\n",
    "def plot_vectors_3d(vectors, model_name):\n",
    "    tsne = TSNE(n_components=3, random_state=0)\n",
    "    vectors_tsne = tsne.fit_transform(vectors)\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1], vectors_tsne[:, 2])\n",
    "    ax.set_title(f'{model_name} t-SNE Plot (3D)')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting all the encoded/vectorized vectors in 3D\n",
    "plot_vectors_3d(one_hot_encoded.toarray(), 'One Hot Encoded')\n",
    "plot_vectors_3d(bag_of_words.toarray(), 'Bag of Words')\n",
    "plot_vectors_3d(tfidf_matrix.toarray(), 'TF-IDF')\n",
    "# Add similar lines for Word2Vec, GloVe, and FastText vectors\n",
    "\n",
    "# Provide a general conclusion\n",
    "print(\"General Conclusion:\")\n",
    "print(\"Word embedding techniques like Word2Vec, GloVe, and FastText capture semantic relationships between words, making them suitable for NLP tasks.\")\n",
    "print(\"TF-IDF is useful for representing the importance of words in a document.\")\n",
    "print(\"Bag of Words and one hot encoding are simpler techniques that do not consider word order but can be effective in certain contexts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b96a5a-0747-4c9e-b4cb-9055e3abbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get all the words in the dataset\n",
    "all_words = set()\n",
    "for article in flattened_data:\n",
    "    all_words.update(word_tokenize(article))\n",
    "\n",
    "# Apply Skip Gram\n",
    "skipgram_vectors = []\n",
    "for word in all_words:\n",
    "    try:\n",
    "        vector = skipgram_model.wv[word]\n",
    "        skipgram_vectors.append(vector)\n",
    "    except KeyError:\n",
    "        pass\n",
    "skipgram_vectors_array = np.array(skipgram_vectors).reshape(-1, 100)\n",
    "\n",
    "# Apply CBOW\n",
    "cbow_vectors = []\n",
    "for word in all_words:\n",
    "    try:\n",
    "        vector = cbow_model.wv[word]\n",
    "        cbow_vectors.append(vector)\n",
    "    except KeyError:\n",
    "        pass\n",
    "cbow_vectors_array = np.array(cbow_vectors).reshape(-1, 100)\n",
    "\n",
    "# Apply Glove\n",
    "glove_vectors = []\n",
    "for word in all_words:\n",
    "    try:\n",
    "        vector = glove_model.wv[word]\n",
    "        glove_vectors.append(vector)\n",
    "    except KeyError:\n",
    "        pass\n",
    "glove_vectors_array = np.array(glove_vectors).reshape(-1, 100)\n",
    "\n",
    "# Apply FastText\n",
    "fasttext_vectors = []\n",
    "for word in all_words:\n",
    "    try:\n",
    "        vector = fasttext_model.wv[word]\n",
    "        fasttext_vectors.append(vector)\n",
    "    except KeyError:\n",
    "        pass\n",
    "fasttext_vectors_array = np.array(fasttext_vectors).reshape(-1, 100)\n",
    "\n",
    "# Concatenate all the vector matrices into one array\n",
    "all_vectors_array = np.concatenate((skipgram_vectors_array, cbow_vectors_array, glove_vectors_array, fasttext_vectors_array))\n",
    "\n",
    "# Apply t-SNE with 3 components for 3D visualization\n",
    "tsne_3d = TSNE(n_components=3, random_state=42)\n",
    "vectors_tsne_3d = tsne_3d.fit_transform(all_vectors_array)\n",
    "\n",
    "# Plot the vectors in 3D\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(vectors_tsne_3d[:, 0], vectors_tsne_3d[:, 1], vectors_tsne_3d[:, 2])\n",
    "for i, word in enumerate(all_words):\n",
    "    ax.text(vectors_tsne_3d[i, 0], vectors_tsne_3d[i, 1], vectors_tsne_3d[i, 2], word, color='black')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('t-SNE Visualization of Word Embeddings in 3D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aeae9c-ceb6-441c-9f69-870c998f4b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a995ae-77d0-4885-9053-fda93c2a46e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
